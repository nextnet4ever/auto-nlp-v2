{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To activate the openIE jar, in the directory where OpenIE standalone is located, you have to run:\n",
    "\n",
    "\n",
    "java -Xmx50g -XX:+UseConcMarkSweepGC -jar openie-assembly-5.0-SNAPSHOT.jar --httpPort 8008"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the contents of REL_HELPERS.py\n",
    "\n",
    "from functools import partial\n",
    "import pandas as pd\n",
    "import sys\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import threading\n",
    "from func_timeout import func_timeout, FunctionTimedOut\n",
    "import traceback\n",
    "\n",
    "from pyopenie import OpenIE5\n",
    "#extractor = OpenIE5('http://localhost:9000')\n",
    "extractors = [OpenIE5('http://localhost:8000'), OpenIE5('http://localhost:9000')]\n",
    "\n",
    "import nltk.data\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "import ctypes\n",
    "\n",
    "\n",
    "total_arash = []\n",
    "total_rows = []\n",
    "\n",
    "\n",
    "\n",
    "def HELPER_to_extract(sent, row, core_NLP_client):# = extractors):# None):\n",
    "    # global total_arash\n",
    "    # global total_rows\n",
    "    total_rows = []\n",
    "    arash = [] # naming one var arash so the legacy lives on...\n",
    "    try:\n",
    "        results = core_NLP_client.annotate(sent)\n",
    "        for res in results:\n",
    "            for arg2 in res[\"extraction\"][\"arg2s\"]:\n",
    "                arash.append([res[\"extraction\"][\"arg1\"][\"text\"], \\\n",
    "                    res[\"extraction\"][\"rel\"][\"text\"], \\\n",
    "                        arg2[\"text\"], res[\"confidence\"], res[\"extraction\"][\"context\"], \\\n",
    "                            res[\"extraction\"][\"negated\"], res[\"extraction\"][\"passive\"]])\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "    # #with lock:\n",
    "    #     total_arash.extend(arash)\n",
    "    for i in range(len(arash)):\n",
    "        total_rows.append(row)\n",
    "    return arash, total_rows\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def HELPER_clean_sentence(text):\n",
    "    # TO DO: Refactor with other punctuations...\n",
    "    text = text.replace('”','\"').replace('’','\\'').replace(',',',').replace('“','\"')\n",
    "    text1 = re.sub(r'[^\\x00-\\x7f]',r'', text) \n",
    "    return text1.replace(\"\\n\",\" \")\n",
    "\n",
    "\n",
    "def HELPER_rels_quick_clean(text):\n",
    "    # expand appostrophes\n",
    "    return text#.replace(\"'s\",\"is\").replace(\"'m\",\"am\").replace(\"'re\",\"are\")\n",
    "\n",
    "# Method to implement extractions(?)\n",
    "def OPTION1(df = None):\n",
    "    # global total_arash\n",
    "    # global total_rows\n",
    "\n",
    "    df_new = pd.DataFrame()\n",
    "    headers = [\"postnum\", \"sentencenum\", \"relnum\", \"arg1\", \"rel\", \"arg2\", \"confidence\", \"context\", \"negated\", \"passive\"]\n",
    "    partial_sents = []\n",
    "    partial_rows = []\n",
    "\n",
    "    for i, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "            \n",
    "            # This does string preprocessing so it's not gigafucked from trying to adapt pdfs to text.\n",
    "            sent = str(row[\"sentence:String\"])\n",
    "            sent1 = sent.replace('”','\"').replace('’','\\'').replace(',',',').replace('“','\"').replace(\"\\n\",\"\")\n",
    "            sent1 = re.sub(r'[^\\x00-\\x7f]',r'', sent1) \n",
    "            if len(sent1) > 300 or \"This file\" in sent1 or \"copyright law\" in sent1 or \"contributed equally\" in sent1 or sent1.count('\\n')>1:\n",
    "                continue\n",
    "\n",
    "            partial_sents.append(sent1)\n",
    "            partial_rows.append(row)\n",
    "\n",
    "\n",
    "            if len(partial_sents) < 1:\n",
    "                continue\n",
    "            else:\n",
    "                # global lock\n",
    "                # lock = threading.Lock()\n",
    "                # threads = [threading.Thread(target=HELPER_to_extract, args=(sent, row, extractor)) for (sent,row,extractor) in zip(partial_sents, partial_rows, extractors)]\n",
    "                # for t in threads: \n",
    "                #     t.start()\n",
    "                # for t in threads:\n",
    "                #     t.join()\n",
    "                try:\n",
    "\n",
    "                    # Here is where the money is made. This is the function that does the relationship extraction. \n",
    "                    total_arash, total_rows = HELPER_to_extract(partial_sents[0], partial_rows[0], extractors[0])\n",
    "                    partial_sents = []\n",
    "                    partial_rows = []\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "                    break#continue\n",
    "\n",
    "            relationships = total_arash\n",
    "            total_arash = []\n",
    "\n",
    "            for k, (rel,r) in enumerate(zip(relationships, total_rows)):\n",
    "                \n",
    "\n",
    "                # This step has some ad-hocs to deal with exceptions we know of.\n",
    "                # TODO: REFACTOR in simpler terms.\n",
    "                dictionary = {}\n",
    "                if rel[0] in [\"Tcell\",\"T-cell\",\"T-cells\",\"Tcells\",\"t cell\",\"t-cell\",\"T cell\", \"T cells\"]:\n",
    "                    rel[0] = \"T-cell\"\n",
    "                if rel[2] in [\"Tcell\",\"T-cell\",\"T-cells\",\"Tcells\",\"t cell\",\"t-cell\",\"T cell\", \"T cells\"]:\n",
    "                    rel[2] = \"T-cell\"\n",
    "\n",
    "\n",
    "                # Constructing the relationships into a format that will eventually be put into a dataframe for neptune upload\n",
    "                dictionary[\"arg1:String\"] = rel[0]\n",
    "                dictionary[\"rel:String\"] = HELPER_rels_quick_clean(rel[1])\n",
    "                dictionary[\"arg2:String\"] = rel[2]\n",
    "                #dictionary[\"context\"] = sent\n",
    "                dictionary[\"confidence:String\"] = rel[3]\n",
    "                dictionary[\"context_rel:String\"] = rel[4]\n",
    "                dictionary[\"negated:String\"] = rel[5]\n",
    "                dictionary[\"passive:String\"] = rel[6]\n",
    "\n",
    "                for k,v in r.items():\n",
    "                    if k not in [\"text\",\"postnum\",\"sentencenum\"]:\n",
    "                        dictionary[k] = v\n",
    "                try:        \n",
    "                    df_new = df_new.append(dictionary, ignore_index=True)\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "                    continue\n",
    "\n",
    "            total_rows = []\n",
    "\n",
    "    #df_new = df_new.reindex(headers, axis=1)\n",
    "    return df_new\n",
    "\n",
    "\n",
    "\n",
    "def kill_bill_and_get_extractions(df):\n",
    "    try:\n",
    "        df_new = OPTION1(df)\n",
    "    except Exception as e:\n",
    "        df_new = pd.DataFrame()\n",
    "        #tb_str = traceback.format_exc()\n",
    "        #print(tb_str)\n",
    "        print(\"Kill bill exception: \")\n",
    "        print(e)\n",
    "    return df_new\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_111913/1798218505.py:13: DeprecationWarning: 'chunk' is deprecated and slated for removal in Python 3.13\n",
      "  import chunk\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('/home/derek/auto-nlp/auto-nlp-bucket/NLP_dump/')\n",
    "\n",
    "from PyPDF2 import PdfReader\n",
    "import PyPDF2\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from scholarly import scholarly\n",
    "from scholarly import ProxyGenerator\n",
    "import pandas as pd\n",
    "import chunk\n",
    "import boto3\n",
    "import nltk\n",
    "from collections import defaultdict\n",
    "import numpy\n",
    "from numpy import real\n",
    "#from REL_HELPERS import *\n",
    "from io import StringIO # python3; python2: BytesIO \n",
    "import boto3\n",
    "import json\n",
    "import gzip\n",
    "import multiprocessing\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import random\n",
    "import string\n",
    "import spacy\n",
    "from celery import Celery\n",
    "from celery.utils.log import get_task_logger\n",
    "import traceback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_sci_lg\")\n",
    "nlp2 = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def approve(i):\n",
    "    words = i.split()\n",
    "    for word in words:\n",
    "        if word in [\"Vol.\",\"Journal\",\"2019\",\"2018\",\"2020\"]:\n",
    "            return False\n",
    "        if \"www\" in word:\n",
    "            return False\n",
    "        if \".com\" in word:\n",
    "            return False\n",
    "        if \".edu\" in word:\n",
    "            return False\n",
    "        if \"License\" in word:\n",
    "            return False\n",
    "        if \"doi\" in word or \"ncbi\" in word or \"Vol.\" in word:\n",
    "            return False\n",
    "    if len(i) < 25:\n",
    "        return False\n",
    "    if i.count(\",\") > 6:\n",
    "        return False\n",
    "    if i.count(\".\") > 2:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "# This method takes text and then returns it as a token of sentences.\n",
    "# From the docs:\n",
    "# \n",
    "# nltk.tokenize.sent_tokenize(text, language='english')\n",
    "#\n",
    "# Return a sentence-tokenized copy of text, using NLTK’s recommended \n",
    "# sentence tokenizer (currently PunktSentenceTokenizer for the \n",
    "# specified language).\n",
    "#\n",
    "#\n",
    "# content = a string containing the text to be parsed\n",
    "# from_path = a data field specific to pubmed publications that has a pointer to where the plaintext \n",
    "#             of that article is\n",
    "def retrieve_metadata(content, from_path = None):\n",
    "\n",
    "    # Get the sentences\n",
    "    sent_text = nltk.sent_tokenize(content)\n",
    "\n",
    "    buffer = \"\"\n",
    "    text = defaultdict(list)\n",
    "\n",
    "    for count, i in enumerate(sent_text):\n",
    "        if approve(i):\n",
    "            if \":\" in i:\n",
    "                sample = i.partition(\":\")[0]\n",
    "                if len(sample) < 20:\n",
    "                    buffer = sample\n",
    "                    i = i.replace(buffer + \":\", \"\")\n",
    "\n",
    "            temp_context = []\n",
    "            if count > 0:\n",
    "                temp_context.append(sent_text[count-1])\n",
    "            temp_context.append(i)\n",
    "            if count < len(sent_text)-1:\n",
    "                temp_context.append(sent_text[count+1])\n",
    "            \n",
    "            text[\"context:String\"].append(\" \".join(temp_context))\n",
    "            text[\"sentence:String\"].append(i)\n",
    "            text[\"tag:String\"].append(buffer)\n",
    "            text[\"path:String\"].append(from_path)  \n",
    "    \n",
    "    df = pd.DataFrame().from_dict(text)\n",
    "    return df\n",
    "\n",
    "def find_key_words(dictionary):\n",
    "    doc = nlp(dictionary[\"sentence:String\"])\n",
    "    entities = doc.ents\n",
    "    real_arg1 = dictionary[\"arg1:String\"].lower()\n",
    "    real_arg2 = dictionary[\"arg2:String\"].lower()\n",
    "    FLAG_first = False\n",
    "    FLAG_second = False\n",
    "    for ent in entities:\n",
    "        if ent.text in dictionary[\"arg1:String\"]: # overwrites because last search words are better\n",
    "            real_arg1 = ent.text\n",
    "            FLAG_first = True\n",
    "        if ent.text in dictionary[\"arg2:String\"]:\n",
    "            real_arg2 = ent.text\n",
    "            FLAG_second = True\n",
    "    doc = nlp2(dictionary[\"sentence:String\"])\n",
    "    for chunk in doc.noun_chunks:\n",
    "        if FLAG_first == False:\n",
    "            if chunk.root.text in dictionary[\"arg1:String\"]:\n",
    "                real_arg1 = nlp2(chunk.root.text)[0].lemma_\n",
    "        \n",
    "        if FLAG_second == False:\n",
    "            if chunk.root.text in dictionary[\"arg2:String\"]:\n",
    "                real_arg2 = nlp2(chunk.root.text)[0].lemma_\n",
    "    try:\n",
    "        if real_arg1[0] == dictionary[\"sentence:String\"][0] and str.islower(real_arg1[1]):\n",
    "            real_arg1 = real_arg1.lower()\n",
    "    except:\n",
    "        pass\n",
    "    if real_arg1 == real_arg2 \\\n",
    "        or real_arg1.lower() in stopwords.words('english') \\\n",
    "        or real_arg2.lower() in stopwords.words('english'):\n",
    "        return False, None, None\n",
    "    else:\n",
    "        return True, real_arg1, real_arg2\n",
    "\n",
    "def return_random_string(N=20):\n",
    "    return ''.join(random.SystemRandom().choice(string.ascii_uppercase + string.digits) for _ in range(N))\n",
    "\n",
    "\n",
    "# This formats data into data frames necessary for functioning\n",
    "def URANUS_redone(results):\n",
    "    df_nodes = {\"~id\":[],\"name:String\":[]}\n",
    "    df_edges = pd.DataFrame()\n",
    "\n",
    "    for i,row in results.iterrows():\n",
    "        FLAG, real_arg1, real_arg2 = find_key_words(row)\n",
    "        if FLAG:\n",
    "\n",
    "            df_nodes[\"~id\"].append(\"LIT=\" + str(real_arg1))\n",
    "            df_nodes[\"name:String\"].append(str(real_arg1))\n",
    "\n",
    "            df_nodes[\"~id\"].append(\"LIT=\" + str(real_arg2))\n",
    "            df_nodes[\"name:String\"].append(str(real_arg2))\n",
    "\n",
    "            temp_row = row\n",
    "            temp_row[\"~from\"] = \"LIT=\" + str(real_arg1)\n",
    "            temp_row[\"~to\"] = \"LIT=\" + str(real_arg2)\n",
    "            temp_row[\"~id\"] = return_random_string() + \"==\" + \"LIT=\" + str(real_arg1) + \"_TO_\" + \"LIT=\" + str(real_arg2)\n",
    "\n",
    "            df_edges = df_edges.append(temp_row, ignore_index=True)\n",
    "    \n",
    "    nodes_all = pd.DataFrame().from_dict(df_nodes)\n",
    "    return nodes_all, df_edges\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint alpha\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/93 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not enough values to unpack (expected 2, got 0)\n",
      "checkpoint bravo\n",
      "(0, 3)\n",
      "(0, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# We now iterate over the pubmed articles\n",
    "\n",
    "total_nodes = pd.DataFrame()\n",
    "total_edges = pd.DataFrame()\n",
    "\n",
    "input_dir = '.'\n",
    "output_dir = '.'\n",
    "filename = 'wealth_of_nations_excerpt.txt'\n",
    "\n",
    "\n",
    "try:\n",
    "    filepath = os.path.join(input_dir, filename)\n",
    "    with open(filepath, 'r') as file:\n",
    "        data_lines = ''.join([line.strip() for line in file.readlines()])\n",
    "\n",
    "    # print(dois.loc[filename[:-4]]['DOI'])\n",
    "    # df = retrieve_metadata(str(data_lines), dois.loc[filename[:-4]]['DOI'])#GET DOI FOR THE PAPER\n",
    "    df = retrieve_metadata(str(data_lines)) \n",
    "    print(\"checkpoint alpha\")\n",
    "    \n",
    "    \n",
    "    # Here is the step for calling the extraction functions\n",
    "    results = kill_bill_and_get_extractions(df)\n",
    "    print(\"checkpoint bravo\")\n",
    "    \n",
    "    \n",
    "    # This is to format the results\n",
    "    temp_node_csv, temp_edge_csv = URANUS_redone(results)\n",
    "    temp_node_csv[\"~label\"] = \"Scientific Literature\"\n",
    "    temp_edge_csv[\"~label\"] = \"Scientific Literature\"\n",
    "    \n",
    "    \n",
    "    print(\"Checkpoint charlie\")\n",
    "\n",
    "\n",
    "    total_nodes = pd.concat([total_nodes, temp_node_csv], axis=0, ignore_index=True)\n",
    "    total_edges = pd.concat([total_edges, temp_edge_csv], axis=0, ignore_index=True)\n",
    "    print(total_nodes.shape)\n",
    "    print(total_edges.shape)\n",
    "    #data_lines.close()\n",
    "    \n",
    "\n",
    "    if total_edges.shape[0] > 0: #remove after testing\n",
    "        # specify the file path and file name\n",
    "        #file_path_nodes = output_dir+'/df_nodes_{filename}.csv'\n",
    "        #file_path_edges = output_dir+'/df_edges_{filename}.csv'\n",
    "\n",
    "        # save the DataFrame to a CSV file\n",
    "        #total_nodes.to_csv(file_path_nodes, index=False)\n",
    "        #total_edges.to_csv(file_path_edges, index=False)\n",
    "\n",
    "\n",
    "        # Save processed data to file in output_dir\n",
    "        output_filename = f\"{filename[:-4]}_processed.csv\" # strip \".txt\" and add \"_processed.csv\"\n",
    "        output_filepath_nodes = os.path.join(output_dir, f\"df_nodes_{output_filename}\")\n",
    "        output_filepath_edges = os.path.join(output_dir, f\"df_edges_{output_filename}\")\n",
    "        total_nodes.to_csv(output_filepath_nodes, index=False)\n",
    "        total_edges.to_csv(output_filepath_edges, index=False)\n",
    "except Exception as e:\n",
    "    print(f\"Error processing {filename}: {e}\")\n",
    "    tb_str = traceback.format_exc()\n",
    "    print(tb_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bioinfo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
