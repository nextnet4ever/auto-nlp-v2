{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import stanza\n",
    "from tqdm.notebook import tqdm\n",
    "os.chdir('/home/derek')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To activate the openIE jar, in the directory where OpenIE standalone is located, you have to run:\n",
    "\n",
    "\n",
    "java -Xmx50g -XX:+UseConcMarkSweepGC -jar openie-assembly-5.0-SNAPSHOT.jar --httpPort 8008"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import pandas as pd\n",
    "import sys\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import threading\n",
    "from func_timeout import func_timeout, FunctionTimedOut\n",
    "import traceback\n",
    "\n",
    "from pyopenie import OpenIE5\n",
    "#extractor = OpenIE5('http://localhost:9000')\n",
    "extractors = [OpenIE5('http://localhost:8000'), OpenIE5('http://localhost:9000')]\n",
    "\n",
    "import nltk.data\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "import ctypes\n",
    "\n",
    "\n",
    "total_arash = []\n",
    "total_rows = []\n",
    "\n",
    "\n",
    "\n",
    "def HELPER_to_extract(sent, row, core_NLP_client):# = extractors):# None):\n",
    "    # global total_arash\n",
    "    # global total_rows\n",
    "    total_rows = []\n",
    "    arash = [] # naming one var arash so the legacy lives on...\n",
    "    try:\n",
    "        results = core_NLP_client.annotate(sent)\n",
    "        for res in results:\n",
    "            for arg2 in res[\"extraction\"][\"arg2s\"]:\n",
    "                arash.append([res[\"extraction\"][\"arg1\"][\"text\"], \\\n",
    "                    res[\"extraction\"][\"rel\"][\"text\"], \\\n",
    "                        arg2[\"text\"], res[\"confidence\"], res[\"extraction\"][\"context\"], \\\n",
    "                            res[\"extraction\"][\"negated\"], res[\"extraction\"][\"passive\"]])\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "    # #with lock:\n",
    "    #     total_arash.extend(arash)\n",
    "    for i in range(len(arash)):\n",
    "        total_rows.append(row)\n",
    "    return arash, total_rows\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def HELPER_clean_sentence(text):\n",
    "    # TO DO: Refactor with other punctuations...\n",
    "    text = text.replace('”','\"').replace('’','\\'').replace(',',',').replace('“','\"')\n",
    "    text1 = re.sub(r'[^\\x00-\\x7f]',r'', text) \n",
    "    return text1.replace(\"\\n\",\" \")\n",
    "\n",
    "\n",
    "def HELPER_rels_quick_clean(text):\n",
    "    # expand appostrophes\n",
    "    return text#.replace(\"'s\",\"is\").replace(\"'m\",\"am\").replace(\"'re\",\"are\")\n",
    "\n",
    "# Method to implement extractions(?)\n",
    "def OPTION1(df = None):\n",
    "    # global total_arash\n",
    "    # global total_rows\n",
    "\n",
    "    df_new = pd.DataFrame()\n",
    "    headers = [\"postnum\", \"sentencenum\", \"relnum\", \"arg1\", \"rel\", \"arg2\", \"confidence\", \"context\", \"negated\", \"passive\"]\n",
    "    partial_sents = []\n",
    "    partial_rows = []\n",
    "\n",
    "    for i, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "            \n",
    "            # This does string preprocessing so it's not gigafucked from trying to adapt pdfs to text.\n",
    "            sent = str(row[\"sentence:String\"])\n",
    "            sent1 = sent.replace('”','\"').replace('’','\\'').replace(',',',').replace('“','\"').replace(\"\\n\",\"\")\n",
    "            sent1 = re.sub(r'[^\\x00-\\x7f]',r'', sent1) \n",
    "            if len(sent1) > 300 or \"This file\" in sent1 or \"copyright law\" in sent1 or \"contributed equally\" in sent1 or sent1.count('\\n')>1:\n",
    "                continue\n",
    "\n",
    "            partial_sents.append(sent1)\n",
    "            partial_rows.append(row)\n",
    "\n",
    "\n",
    "            if len(partial_sents) < 1:\n",
    "                continue\n",
    "            else:\n",
    "                # global lock\n",
    "                # lock = threading.Lock()\n",
    "                # threads = [threading.Thread(target=HELPER_to_extract, args=(sent, row, extractor)) for (sent,row,extractor) in zip(partial_sents, partial_rows, extractors)]\n",
    "                # for t in threads: \n",
    "                #     t.start()\n",
    "                # for t in threads:\n",
    "                #     t.join()\n",
    "                try:\n",
    "\n",
    "                    # Here is where the money is made. This is the function that does the relationship extraction. \n",
    "                    total_arash, total_rows = HELPER_to_extract(partial_sents[0], partial_rows[0], extractors[0])\n",
    "                    partial_sents = []\n",
    "                    partial_rows = []\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "                    break#continue\n",
    "\n",
    "            relationships = total_arash\n",
    "            total_arash = []\n",
    "\n",
    "            for k, (rel,r) in enumerate(zip(relationships, total_rows)):\n",
    "                \n",
    "\n",
    "                # This step has some ad-hocs to deal with exceptions we know of.\n",
    "                # TODO: REFACTOR in simpler terms.\n",
    "                dictionary = {}\n",
    "                if rel[0] in [\"Tcell\",\"T-cell\",\"T-cells\",\"Tcells\",\"t cell\",\"t-cell\",\"T cell\", \"T cells\"]:\n",
    "                    rel[0] = \"T-cell\"\n",
    "                if rel[2] in [\"Tcell\",\"T-cell\",\"T-cells\",\"Tcells\",\"t cell\",\"t-cell\",\"T cell\", \"T cells\"]:\n",
    "                    rel[2] = \"T-cell\"\n",
    "\n",
    "\n",
    "                # Constructing the relationships into a format that will eventually be put into a dataframe for neptune upload\n",
    "                dictionary[\"arg1:String\"] = rel[0]\n",
    "                dictionary[\"rel:String\"] = HELPER_rels_quick_clean(rel[1])\n",
    "                dictionary[\"arg2:String\"] = rel[2]\n",
    "                #dictionary[\"context\"] = sent\n",
    "                dictionary[\"confidence:String\"] = rel[3]\n",
    "                dictionary[\"context_rel:String\"] = rel[4]\n",
    "                dictionary[\"negated:String\"] = rel[5]\n",
    "                dictionary[\"passive:String\"] = rel[6]\n",
    "\n",
    "                for k,v in r.items():\n",
    "                    if k not in [\"text\",\"postnum\",\"sentencenum\"]:\n",
    "                        dictionary[k] = v\n",
    "                try:        \n",
    "                    df_new = df_new.append(dictionary, ignore_index=True)\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "                    continue\n",
    "\n",
    "            total_rows = []\n",
    "\n",
    "    #df_new = df_new.reindex(headers, axis=1)\n",
    "    return df_new\n",
    "\n",
    "\n",
    "\n",
    "def kill_bill_and_get_extractions(df):\n",
    "    try:\n",
    "        df_new = OPTION1(df)\n",
    "    except Exception as e:\n",
    "        df_new = pd.DataFrame()\n",
    "        #tb_str = traceback.format_exc()\n",
    "        #print(tb_str)\n",
    "        print(e)\n",
    "    return df_new\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'REL_HELPERS'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mimport\u001b[39;00m real\n\u001b[0;32m---> 19\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mREL_HELPERS\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[1;32m     20\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mio\u001b[39;00m \u001b[39mimport\u001b[39;00m StringIO \u001b[39m# python3; python2: BytesIO \u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mboto3\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'REL_HELPERS'"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('/home/derek/auto-nlp/auto-nlp-bucket/NLP_dump/')\n",
    "\n",
    "from PyPDF2 import PdfReader\n",
    "import PyPDF2\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from scholarly import scholarly\n",
    "from scholarly import ProxyGenerator\n",
    "import pandas as pd\n",
    "import chunk\n",
    "import boto3\n",
    "import nltk\n",
    "from collections import defaultdict\n",
    "import numpy\n",
    "from numpy import real\n",
    "from REL_HELPERS import *\n",
    "from io import StringIO # python3; python2: BytesIO \n",
    "import boto3\n",
    "import json\n",
    "import gzip\n",
    "import multiprocessing\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import random\n",
    "import string\n",
    "import spacy\n",
    "from celery import Celery\n",
    "from celery.utils.log import get_task_logger\n",
    "import traceback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now iterate over the pubmed articles\n",
    "\n",
    "total_nodes = pd.DataFrame()\n",
    "total_edges = pd.DataFrame()\n",
    "try:\n",
    "    filepath = os.path.join(input_dir, filename)\n",
    "    with open(filepath, 'r') as file:\n",
    "        data_lines = ''.join([line.strip() for line in file.readlines()])\n",
    "\n",
    "    print(dois.loc[filename[:-4]]['DOI'])\n",
    "    df = retrieve_metadata(str(data_lines), dois.loc[filename[:-4]]['DOI'])#GET DOI FOR THE PAPER\n",
    "\n",
    "    # Here is the step for calling the extraction functions\n",
    "    results = kill_bill_and_get_extractions(df)\n",
    "    \n",
    "    # This is to format the results\n",
    "    temp_node_csv, temp_edge_csv = URANUS_redone(results)\n",
    "    temp_node_csv[\"~label\"] = \"Scientific Literature\"\n",
    "    temp_edge_csv[\"~label\"] = \"Scientific Literature\"\n",
    "\n",
    "    total_nodes = total_nodes.append(temp_node_csv, ignore_index=True)\n",
    "    total_edges = total_edges.append(temp_edge_csv, ignore_index=True)\n",
    "    print(total_nodes.shape)\n",
    "    print(total_edges.shape)\n",
    "    #data_lines.close()\n",
    "    \n",
    "\n",
    "    if total_edges.shape[0] > 0: #remove after testing\n",
    "        # specify the file path and file name\n",
    "        #file_path_nodes = output_dir+'/df_nodes_{filename}.csv'\n",
    "        #file_path_edges = output_dir+'/df_edges_{filename}.csv'\n",
    "\n",
    "        # save the DataFrame to a CSV file\n",
    "        #total_nodes.to_csv(file_path_nodes, index=False)\n",
    "        #total_edges.to_csv(file_path_edges, index=False)\n",
    "\n",
    "\n",
    "        # Save processed data to file in output_dir\n",
    "        output_filename = f\"{filename[:-4]}_processed.csv\" # strip \".txt\" and add \"_processed.csv\"\n",
    "        output_filepath_nodes = os.path.join(output_dir, f\"df_nodes_{output_filename}\")\n",
    "        output_filepath_edges = os.path.join(output_dir, f\"df_edges_{output_filename}\")\n",
    "        total_nodes.to_csv(output_filepath_nodes, index=False)\n",
    "        total_edges.to_csv(output_filepath_edges, index=False)\n",
    "except Exception as e:\n",
    "    print(f\"Error processing {filename}: {e}\")\n",
    "    tb_str = traceback.format_exc()\n",
    "    print(tb_str)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bioinfo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
