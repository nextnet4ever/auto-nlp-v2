{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this notebook is to prototype how OpenIE6 will be used to annotate raw text information\n",
    "\n",
    "For a fuller explanation, go and see the references here on Notion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n"
     ]
    }
   ],
   "source": [
    "# Download data in the form of pubmed files and convert it to text\n",
    "from lit_data import pdf_to_text\n",
    "\n",
    "pdf_to_text.process_query_item('test') \n",
    "\n",
    "\n",
    "# By now we should have text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn it into sentences\n",
    "\n",
    "with open('text/nihms-1536901.txt', 'r') as f:\n",
    "\n",
    "    total_string = \"\"\n",
    "\n",
    "    for line in f:\n",
    "\n",
    "        total_string += line\n",
    "    \n",
    "\n",
    "    # Now strip the linebreaks\n",
    "\n",
    "    total_string = total_string.replace(\"\\n\", \"\")\n",
    "\n",
    "    total_string = total_string.split(\".\")\n",
    "\n",
    "\n",
    "    with open('text/nihms-1536901-sentences.txt', 'w') as f2:\n",
    "\n",
    "        for sentence in total_string:\n",
    "            #print(sentence +'.' + '\\n')\n",
    "            f2.write(sentence + '.' + '\\n')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# Preprocess the data\n",
    "\n",
    "# Imports for preprocessing\n",
    "\n",
    "import os\n",
    "from PyPDF2 import PdfReader\n",
    "import PyPDF2\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from scholarly import scholarly\n",
    "from scholarly import ProxyGenerator\n",
    "import pandas as pd\n",
    "import chunk\n",
    "import boto3\n",
    "import nltk\n",
    "from collections import defaultdict\n",
    "import numpy\n",
    "from numpy import real\n",
    "\n",
    "from io import StringIO # python3; python2: BytesIO \n",
    "import boto3\n",
    "import json\n",
    "import gzip\n",
    "import multiprocessing\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import random\n",
    "import string\n",
    "import spacy\n",
    "from celery import Celery\n",
    "from celery.utils.log import get_task_logger\n",
    "import traceback\n",
    "\n",
    "nlp = spacy.load(\"en_core_sci_lg\")\n",
    "nlp2 = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "\n",
    "\n",
    "def approve(i):\n",
    "    words = i.split()\n",
    "    for word in words:\n",
    "        if word in [\"Vol.\",\"Journal\",\"2019\",\"2018\",\"2020\"]:\n",
    "            return False\n",
    "        if \"www\" in word:\n",
    "            return False\n",
    "        if \".com\" in word:\n",
    "            return False\n",
    "        if \".edu\" in word:\n",
    "            return False\n",
    "        if \"License\" in word:\n",
    "            return False\n",
    "        if \"doi\" in word or \"ncbi\" in word or \"Vol.\" in word:\n",
    "            return False\n",
    "    if len(i) < 25:\n",
    "        return False\n",
    "    if i.count(\",\") > 6:\n",
    "        return False\n",
    "    if i.count(\".\") > 2:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "# This method takes text and then returns it as a token of sentences.\n",
    "# From the docs:\n",
    "# \n",
    "# nltk.tokenize.sent_tokenize(text, language='english')\n",
    "#\n",
    "# Return a sentence-tokenized copy of text, using NLTKâ€™s recommended \n",
    "# sentence tokenizer (currently PunktSentenceTokenizer for the \n",
    "# specified language).\n",
    "#\n",
    "#\n",
    "# content = a string containing the text to be parsed\n",
    "# from_path = a data field specific to pubmed publications that has a pointer to where the plaintext \n",
    "#             of that article is\n",
    "def retrieve_metadata(content, from_path = None):\n",
    "\n",
    "    # Get the sentences\n",
    "    sent_text = nltk.sent_tokenize(content)\n",
    "\n",
    "    buffer = \"\"\n",
    "    text = defaultdict(list)\n",
    "\n",
    "    for count, i in enumerate(sent_text):\n",
    "        if approve(i):\n",
    "            if \":\" in i:\n",
    "                sample = i.partition(\":\")[0]\n",
    "                if len(sample) < 20:\n",
    "                    buffer = sample\n",
    "                    i = i.replace(buffer + \":\", \"\")\n",
    "\n",
    "            temp_context = []\n",
    "            if count > 0:\n",
    "                temp_context.append(sent_text[count-1])\n",
    "            temp_context.append(i)\n",
    "            if count < len(sent_text)-1:\n",
    "                temp_context.append(sent_text[count+1])\n",
    "            \n",
    "            text[\"context:String\"].append(\" \".join(temp_context))\n",
    "            text[\"sentence:String\"].append(i)\n",
    "            text[\"tag:String\"].append(buffer)\n",
    "            text[\"path:String\"].append(from_path)  \n",
    "    \n",
    "    df = pd.DataFrame().from_dict(text)\n",
    "    return df\n",
    "\n",
    "def find_key_words(dictionary):\n",
    "    doc = nlp(dictionary[\"sentence:String\"])\n",
    "    entities = doc.ents\n",
    "    real_arg1 = dictionary[\"arg1:String\"].lower()\n",
    "    real_arg2 = dictionary[\"arg2:String\"].lower()\n",
    "    FLAG_first = False\n",
    "    FLAG_second = False\n",
    "    for ent in entities:\n",
    "        if ent.text in dictionary[\"arg1:String\"]: # overwrites because last search words are better\n",
    "            real_arg1 = ent.text\n",
    "            FLAG_first = True\n",
    "        if ent.text in dictionary[\"arg2:String\"]:\n",
    "            real_arg2 = ent.text\n",
    "            FLAG_second = True\n",
    "    doc = nlp2(dictionary[\"sentence:String\"])\n",
    "    for chunk in doc.noun_chunks:\n",
    "        if FLAG_first == False:\n",
    "            if chunk.root.text in dictionary[\"arg1:String\"]:\n",
    "                real_arg1 = nlp2(chunk.root.text)[0].lemma_\n",
    "        \n",
    "        if FLAG_second == False:\n",
    "            if chunk.root.text in dictionary[\"arg2:String\"]:\n",
    "                real_arg2 = nlp2(chunk.root.text)[0].lemma_\n",
    "    try:\n",
    "        if real_arg1[0] == dictionary[\"sentence:String\"][0] and str.islower(real_arg1[1]):\n",
    "            real_arg1 = real_arg1.lower()\n",
    "    except:\n",
    "        pass\n",
    "    if real_arg1 == real_arg2 \\\n",
    "        or real_arg1.lower() in stopwords.words('english') \\\n",
    "        or real_arg2.lower() in stopwords.words('english'):\n",
    "        return False, None, None\n",
    "    else:\n",
    "        return True, real_arg1, real_arg2\n",
    "\n",
    "def return_random_string(N=20):\n",
    "    return ''.join(random.SystemRandom().choice(string.ascii_uppercase + string.digits) for _ in range(N))\n",
    "\n",
    "\n",
    "# This formats data into data frames necessary for functioning\n",
    "def URANUS_redone(results):\n",
    "    df_nodes = {\"~id\":[],\"name:String\":[]}\n",
    "    df_edges = pd.DataFrame()\n",
    "\n",
    "    for i,row in results.iterrows():\n",
    "        FLAG, real_arg1, real_arg2 = find_key_words(row)\n",
    "        if FLAG:\n",
    "\n",
    "            df_nodes[\"~id\"].append(\"LIT=\" + str(real_arg1))\n",
    "            df_nodes[\"name:String\"].append(str(real_arg1))\n",
    "\n",
    "            df_nodes[\"~id\"].append(\"LIT=\" + str(real_arg2))\n",
    "            df_nodes[\"name:String\"].append(str(real_arg2))\n",
    "\n",
    "            temp_row = row\n",
    "            temp_row[\"~from\"] = \"LIT=\" + str(real_arg1)\n",
    "            temp_row[\"~to\"] = \"LIT=\" + str(real_arg2)\n",
    "            temp_row[\"~id\"] = return_random_string() + \"==\" + \"LIT=\" + str(real_arg1) + \"_TO_\" + \"LIT=\" + str(real_arg2)\n",
    "\n",
    "            df_edges = df_edges.append(temp_row, ignore_index=True)\n",
    "    \n",
    "    nodes_all = pd.DataFrame().from_dict(df_nodes)\n",
    "    return nodes_all, df_edges\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/326 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/326 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metadata retrieved\n",
      "Kill bill called\n",
      "Extractor called\n",
      "not enough values to unpack (expected 2, got 0)\n",
      "kill bill done\n",
      "(0, 3)\n",
      "(0, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "input_dir = \"text\"\n",
    "from lit_data.REL_HELPERS import *\n",
    "for filename in os.listdir(input_dir):\n",
    "    total_nodes = pd.DataFrame()\n",
    "    total_edges = pd.DataFrame()\n",
    "    try:\n",
    "        filepath = os.path.join(input_dir, filename)\n",
    "        with open(filepath, 'r') as file:\n",
    "            data_lines = ''.join([line.strip() for line in file.readlines()])\n",
    "\n",
    "        #print(dois.loc[filename[:-4]]['DOI'])\n",
    "        #df = retrieve_metadata(str(data_lines), dois.loc[filename[:-4]]['DOI'])#GET DOI FOR THE PAPER\n",
    "        df = retrieve_metadata(str(data_lines)) \n",
    "        print('metadata retrieved')\n",
    "        # Here is the step for calling the extraction functions\n",
    "        results = kill_bill_and_get_extractions(df)\n",
    "        print(\"kill bill done\")\n",
    "        \n",
    "        # This is to format the results\n",
    "        temp_node_csv, temp_edge_csv = URANUS_redone(results)\n",
    "        temp_node_csv[\"~label\"] = \"Scientific Literature\"\n",
    "        temp_edge_csv[\"~label\"] = \"Scientific Literature\"\n",
    "\n",
    "        total_nodes = total_nodes.append(temp_node_csv, ignore_index=True)\n",
    "        total_edges = total_edges.append(temp_edge_csv, ignore_index=True)\n",
    "        print(total_nodes.shape)\n",
    "        print(total_edges.shape)\n",
    "        #data_lines.close()\n",
    "        \n",
    "\n",
    "        if total_edges.shape[0] > 0: #remove after testing\n",
    "            # specify the file path and file name\n",
    "            #file_path_nodes = output_dir+'/df_nodes_{filename}.csv'\n",
    "            #file_path_edges = output_dir+'/df_edges_{filename}.csv'\n",
    "\n",
    "            # save the DataFrame to a CSV file\n",
    "            #total_nodes.to_csv(file_path_nodes, index=False)\n",
    "            #total_edges.to_csv(file_path_edges, index=False)\n",
    "            # Save processed data to file in output_dir\n",
    "            output_filename = f\"{filename[:-4]}_processed.csv\" # strip \".txt\" and add \"_processed.csv\"\n",
    "            output_filepath_nodes = os.path.join(output_dir, f\"df_nodes_{output_filename}\")\n",
    "            output_filepath_edges = os.path.join(output_dir, f\"df_edges_{output_filename}\")\n",
    "            total_nodes.to_csv(output_filepath_nodes, index=False)\n",
    "            total_edges.to_csv(output_filepath_edges, index=False)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {filename}: {e}\")\n",
    "        tb_str = traceback.format_exc()\n",
    "        print(tb_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the OpenIE6 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openie6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
